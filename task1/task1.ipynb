{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForTokenClassification\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initializing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "dataset = [\n",
        "    (\"Mount Everest is the highest peak in the world.\", {\"entities\": [(0, 13, \"Mountain\")]}),\n",
        "    (\"Kilimanjaro is the tallest mountain in Africa.\", {\"entities\": [(0, 11, \"Mountain\")]}),\n",
        "    (\"Denali, also known as Mount McKinley, is in North America.\", {\"entities\": [(0, 6, \"Mountain\"), (21, 35, \"Mountain\")]}),\n",
        "    (\"The Matterhorn is a famous mountain in the Alps.\", {\"entities\": [(4, 14, \"Mountain\")]}),\n",
        "    (\"Mount Fuji is an iconic volcano in Japan.\", {\"entities\": [(0, 10, \"Mountain\")]}),\n",
        "    (\"The Rocky Mountains stretch across North America.\", {\"entities\": [(4, 18, \"Mountain\")]}),\n",
        "    (\"The Himalayas are a vast mountain range in Asia.\", {\"entities\": [(4, 12, \"Mountain\")]}),\n",
        "    (\"Mount Elbrus is the highest mountain in Europe.\", {\"entities\": [(0, 11, \"Mountain\")]})\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preparing dataset by separating it into two lists: one containing the sentences and the other containing the corresponding named entity annotations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "sentences = []\n",
        "labels = []\n",
        "for sentence, label in dataset:\n",
        "    sentences.append(sentence)\n",
        "    labels.append(label['entities'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading the pre-trained tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tokenization using the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def tokenize_and_preserve_labels(sentence, text_labels):\n",
        "    tokenized_sentence = []\n",
        "    labels = []\n",
        "\n",
        "    for word, label in zip(sentence, text_labels):\n",
        "        tokenized_word = tokenizer.tokenize(word)\n",
        "        n_subwords = len(tokenized_word)\n",
        "        tokenized_sentence.extend(tokenized_word)\n",
        "        labels.extend([label] * n_subwords)\n",
        "\n",
        "    return tokenized_sentence, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenized_texts_and_labels = [tokenize_and_preserve_labels(sent, labs) for sent, labs in zip(sentences, labels)]\n",
        "tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n",
        "labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Convertion of labels to a binary format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_labels = []\n",
        "for entities in labels:\n",
        "    model_label = [0] * len(tokenized_texts)  \n",
        "    for start, end, _ in entities:\n",
        "        for position in range(len(tokenized_texts)):\n",
        "            if start <= position < end:\n",
        "                model_label[position] = 1\n",
        "    model_labels.append(model_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "model_labels_flat = [label for sublist in model_labels for label in sublist]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creation of mapping from labels to indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "tag2idx = {tag: idx for idx, tag in enumerate(set(model_labels_flat))}\n",
        "tag2idx[\"PAD\"] = len(tag2idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Convertion to tensor format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_ids = [tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts]\n",
        "padded_input_ids = pad_sequence([torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creation of attention masks to ignore padded tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "attention_masks = [[float(i != 0.0) for i in ii] for ii in padded_input_ids]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Splitting data into train and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(padded_input_ids, model_labels, random_state=42, test_size=0.1)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, padded_input_ids, random_state=42, test_size=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Convertion to PyTorch tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create TensorDatasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = TensorDataset(train_inputs, train_labels, train_masks)\n",
        "validation_dataset = TensorDataset(validation_inputs, validation_labels, validation_masks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creation of DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 2\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fine-tuning setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "epoches = 5\n",
        "optimizer = AdamW(model.parameters(), lr=3e-5, eps=1e-8)\n",
        "total_steps = len(train_dataloader) * epoches\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for _ in range(epoches):\n",
        "    model.train()\n",
        "    for batch in train_dataloader:\n",
        "        b_input_ids, b_labels, b_masks = batch\n",
        "        print(b_input_ids)\n",
        "        print(b_labels)\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_masks, labels=b_labels)\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        model.zero_grad()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ner_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "model.save_pretrained(\"./ner_model\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
