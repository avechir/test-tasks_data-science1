{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "pip install torch",
      "metadata": {
        "trusted": true
      },
      "execution_count": 13,
      "outputs": [
        {
          "ename": "<class 'AttributeError'>",
          "evalue": "module 'pexpect' has no attribute 'TIMEOUT'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minstall torch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2456\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2454\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2455\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2456\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2458\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2459\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2460\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
            "File \u001b[0;32m/lib/python3.11/site-packages/IPython/core/magics/packaging.py:92\u001b[0m, in \u001b[0;36mPackagingMagics.pip\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     python \u001b[38;5;241m=\u001b[39m shlex\u001b[38;5;241m.\u001b[39mquote(python)\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpython\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-m\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNote: you may need to restart the kernel to use updated packages.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2629\u001b[0m, in \u001b[0;36mInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m   2624\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackground processes not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2626\u001b[0m \u001b[38;5;66;03m# we explicitly do NOT return the subprocess status code, because\u001b[39;00m\n\u001b[1;32m   2627\u001b[0m \u001b[38;5;66;03m# a non-None value would trigger :func:`sys.displayhook` calls.\u001b[39;00m\n\u001b[1;32m   2628\u001b[0m \u001b[38;5;66;03m# Instead, we store the exit_code in user_ns.\u001b[39;00m\n\u001b[0;32m-> 2629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/lib/python3.11/site-packages/IPython/utils/_process_posix.py:129\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    125\u001b[0m enc \u001b[38;5;241m=\u001b[39m DEFAULT_ENCODING\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Patterns to match on the output, for pexpect.  We read input and\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# allow either a short timeout or EOF\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m patterns \u001b[38;5;241m=\u001b[39m [\u001b[43mpexpect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTIMEOUT\u001b[49m, pexpect\u001b[38;5;241m.\u001b[39mEOF]\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# the index of the EOF pattern in the list.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# even though we know it's 1, this call means we don't have to worry if\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# we change the above list, and forget to change this value:\u001b[39;00m\n\u001b[1;32m    133\u001b[0m EOF_index \u001b[38;5;241m=\u001b[39m patterns\u001b[38;5;241m.\u001b[39mindex(pexpect\u001b[38;5;241m.\u001b[39mEOF)\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'pexpect' has no attribute 'TIMEOUT'"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "pip install transformers",
      "metadata": {
        "trusted": true
      },
      "execution_count": 14,
      "outputs": [
        {
          "ename": "<class 'AttributeError'>",
          "evalue": "module 'pexpect' has no attribute 'TIMEOUT'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpip\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minstall transformers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2456\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2454\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2455\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2456\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2458\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2459\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2460\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
            "File \u001b[0;32m/lib/python3.11/site-packages/IPython/core/magics/packaging.py:92\u001b[0m, in \u001b[0;36mPackagingMagics.pip\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     python \u001b[38;5;241m=\u001b[39m shlex\u001b[38;5;241m.\u001b[39mquote(python)\n\u001b[0;32m---> 92\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpython\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-m\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNote: you may need to restart the kernel to use updated packages.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2629\u001b[0m, in \u001b[0;36mInteractiveShell.system_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m   2624\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackground processes not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2626\u001b[0m \u001b[38;5;66;03m# we explicitly do NOT return the subprocess status code, because\u001b[39;00m\n\u001b[1;32m   2627\u001b[0m \u001b[38;5;66;03m# a non-None value would trigger :func:`sys.displayhook` calls.\u001b[39;00m\n\u001b[1;32m   2628\u001b[0m \u001b[38;5;66;03m# Instead, we store the exit_code in user_ns.\u001b[39;00m\n\u001b[0;32m-> 2629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_exit_code\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvar_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/lib/python3.11/site-packages/IPython/utils/_process_posix.py:129\u001b[0m, in \u001b[0;36mProcessHandler.system\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    125\u001b[0m enc \u001b[38;5;241m=\u001b[39m DEFAULT_ENCODING\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Patterns to match on the output, for pexpect.  We read input and\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# allow either a short timeout or EOF\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m patterns \u001b[38;5;241m=\u001b[39m [\u001b[43mpexpect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTIMEOUT\u001b[49m, pexpect\u001b[38;5;241m.\u001b[39mEOF]\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# the index of the EOF pattern in the list.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# even though we know it's 1, this call means we don't have to worry if\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# we change the above list, and forget to change this value:\u001b[39;00m\n\u001b[1;32m    133\u001b[0m EOF_index \u001b[38;5;241m=\u001b[39m patterns\u001b[38;5;241m.\u001b[39mindex(pexpect\u001b[38;5;241m.\u001b[39mEOF)\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'pexpect' has no attribute 'TIMEOUT'"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "pip install sklearn",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Loading libraries",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import torch\nfrom transformers import BertTokenizer, BertForTokenClassification\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.nn.utils.rnn import pad_sequence\nfrom sklearn.model_selection import train_test_split",
      "metadata": {
        "trusted": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "ename": "<class 'ModuleNotFoundError'>",
          "evalue": "No module named 'torch'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertForTokenClassification\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdamW, get_linear_schedule_with_warmup\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "Initializing dataset",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "dataset = [\n    (\"Mount Everest is the highest peak in the world.\", {\"entities\": [(0, 13, \"Mountain\")]}),\n    (\"Kilimanjaro is the tallest mountain in Africa.\", {\"entities\": [(0, 11, \"Mountain\")]}),\n    (\"Denali, also known as Mount McKinley, is in North America.\", {\"entities\": [(0, 6, \"Mountain\"), (21, 35, \"Mountain\")]}),\n    (\"The Matterhorn is a famous mountain in the Alps.\", {\"entities\": [(4, 14, \"Mountain\")]}),\n    (\"Mount Fuji is an iconic volcano in Japan.\", {\"entities\": [(0, 10, \"Mountain\")]}),\n    (\"The Rocky Mountains stretch across North America.\", {\"entities\": [(4, 18, \"Mountain\")]}),\n    (\"The Himalayas are a vast mountain range in Asia.\", {\"entities\": [(4, 12, \"Mountain\")]}),\n    (\"Mount Elbrus is the highest mountain in Europe.\", {\"entities\": [(0, 11, \"Mountain\")]})\n]",
      "metadata": {
        "trusted": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Preparing dataset by separating it into two lists: one containing the sentences and the other containing the corresponding named entity annotations.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "sentences = []\nlabels = []\nfor sentence, label in dataset:\n    sentences.append(sentence)\n    labels.append(label['entities'])",
      "metadata": {
        "trusted": true
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Loading the pre-trained tokenizer",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")",
      "metadata": {
        "trusted": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'BertTokenizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mBertTokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BertTokenizer' is not defined"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "Tokenization using the tokenizer",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def tokenize_and_preserve_labels(sentence, text_labels):\n    tokenized_sentence = []\n    labels = []\n\n    for word, label in zip(sentence, text_labels):\n        tokenized_word = tokenizer.tokenize(word)\n        n_subwords = len(tokenized_word)\n        tokenized_sentence.extend(tokenized_word)\n        labels.extend([label] * n_subwords)\n\n    return tokenized_sentence, labels",
      "metadata": {
        "trusted": true
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "tokenized_texts_and_labels = [tokenize_and_preserve_labels(sent, labs) for sent, labs in zip(sentences, labels)]\ntokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\nlabels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]",
      "metadata": {
        "trusted": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'tokenizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenized_texts_and_labels \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mtokenize_and_preserve_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      2\u001b[0m tokenized_texts \u001b[38;5;241m=\u001b[39m [token_label_pair[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m token_label_pair \u001b[38;5;129;01min\u001b[39;00m tokenized_texts_and_labels]\n\u001b[1;32m      3\u001b[0m labels \u001b[38;5;241m=\u001b[39m [token_label_pair[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m token_label_pair \u001b[38;5;129;01min\u001b[39;00m tokenized_texts_and_labels]\n",
            "Cell \u001b[0;32mIn[8], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenized_texts_and_labels \u001b[38;5;241m=\u001b[39m [\u001b[43mtokenize_and_preserve_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sent, labs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sentences, labels)]\n\u001b[1;32m      2\u001b[0m tokenized_texts \u001b[38;5;241m=\u001b[39m [token_label_pair[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m token_label_pair \u001b[38;5;129;01min\u001b[39;00m tokenized_texts_and_labels]\n\u001b[1;32m      3\u001b[0m labels \u001b[38;5;241m=\u001b[39m [token_label_pair[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m token_label_pair \u001b[38;5;129;01min\u001b[39;00m tokenized_texts_and_labels]\n",
            "Cell \u001b[0;32mIn[7], line 6\u001b[0m, in \u001b[0;36mtokenize_and_preserve_labels\u001b[0;34m(sentence, text_labels)\u001b[0m\n\u001b[1;32m      3\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sentence, text_labels):\n\u001b[0;32m----> 6\u001b[0m     tokenized_word \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mtokenize(word)\n\u001b[1;32m      7\u001b[0m     n_subwords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokenized_word)\n\u001b[1;32m      8\u001b[0m     tokenized_sentence\u001b[38;5;241m.\u001b[39mextend(tokenized_word)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "Convertion of labels to a binary format",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "model_labels = []\nfor entities in labels:\n    model_label = [0] * len(tokenized_texts)  \n    for start, end, _ in entities:\n        for position in range(len(tokenized_texts)):\n            if start <= position < end:\n                model_label[position] = 1\n    model_labels.append(model_label)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 15,
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'tokenized_texts' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entities \u001b[38;5;129;01min\u001b[39;00m labels:\n\u001b[0;32m----> 3\u001b[0m     model_label \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtokenized_texts\u001b[49m)  \n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m start, end, _ \u001b[38;5;129;01min\u001b[39;00m entities:\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m position \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tokenized_texts)):\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenized_texts' is not defined"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "model_labels_flat = [label for sublist in model_labels for label in sublist]",
      "metadata": {
        "trusted": true
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Creation of mapping from labels to indexes",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "tag2idx = {tag: idx for idx, tag in enumerate(set(model_labels_flat))}\ntag2idx[\"PAD\"] = len(tag2idx)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Convertion to tensor format",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "input_ids = [tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts]\npadded_input_ids = pad_sequence([torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=0)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 18,
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'tokenized_texts' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(txt) \u001b[38;5;28;01mfor\u001b[39;00m txt \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtokenized_texts\u001b[49m]\n\u001b[1;32m      2\u001b[0m padded_input_ids \u001b[38;5;241m=\u001b[39m pad_sequence([torch\u001b[38;5;241m.\u001b[39mtensor(ids) \u001b[38;5;28;01mfor\u001b[39;00m ids \u001b[38;5;129;01min\u001b[39;00m input_ids], batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenized_texts' is not defined"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "Creation of attention masks to ignore padded tokens",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "attention_masks = [[float(i != 0.0) for i in ii] for ii in padded_input_ids]",
      "metadata": {
        "trusted": true
      },
      "execution_count": 19,
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'padded_input_ids' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m attention_masks \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;28mfloat\u001b[39m(i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0.0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ii] \u001b[38;5;28;01mfor\u001b[39;00m ii \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpadded_input_ids\u001b[49m]\n",
            "\u001b[0;31mNameError\u001b[0m: name 'padded_input_ids' is not defined"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "Splitting data into train and validation sets",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(padded_input_ids, model_labels, random_state=42, test_size=0.1)\ntrain_masks, validation_masks, _, _ = train_test_split(attention_masks, padded_input_ids, random_state=42, test_size=0.1)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 20,
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'train_test_split' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_inputs, validation_inputs, train_labels, validation_labels \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m(padded_input_ids, model_labels, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m      2\u001b[0m train_masks, validation_masks, _, _ \u001b[38;5;241m=\u001b[39m train_test_split(attention_masks, padded_input_ids, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "Convertion to PyTorch tensors",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train_inputs = torch.tensor(train_inputs)\nvalidation_inputs = torch.tensor(validation_inputs)\ntrain_labels = torch.tensor(train_labels)\nvalidation_labels = torch.tensor(validation_labels)\ntrain_masks = torch.tensor(train_masks)\nvalidation_masks = torch.tensor(validation_masks)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 21,
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mtensor(train_inputs)\n\u001b[1;32m      2\u001b[0m validation_inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(validation_inputs)\n\u001b[1;32m      3\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(train_labels)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "Create TensorDatasets",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train_dataset = TensorDataset(train_inputs, train_labels, train_masks)\nvalidation_dataset = TensorDataset(validation_inputs, validation_labels, validation_masks)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 22,
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'TensorDataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTensorDataset\u001b[49m(train_inputs, train_labels, train_masks)\n\u001b[1;32m      2\u001b[0m validation_dataset \u001b[38;5;241m=\u001b[39m TensorDataset(validation_inputs, validation_labels, validation_masks)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'TensorDataset' is not defined"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "Creation of DataLoaders",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "batch_size = 2\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nvalidation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 23,
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'DataLoader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m----> 2\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m validation_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(validation_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "Model loading",
      "metadata": {
        "trusted": true
      },
      "execution_count": 24,
      "outputs": [
        {
          "ename": "<class 'SyntaxError'>",
          "evalue": "invalid syntax (<ipython-input-24-e825788b2632>, line 1)",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[24], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Model loading\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 25,
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'BertForTokenClassification' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBertForTokenClassification\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BertForTokenClassification' is not defined"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "Fine-tuning setup",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "epoches = 5\noptimizer = AdamW(model.parameters(), lr=3e-5, eps=1e-8)\ntotal_steps = len(train_dataloader) * epoches\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)",
      "metadata": {
        "trusted": true
      },
      "execution_count": 26,
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'AdamW' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m epoches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m----> 2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mAdamW\u001b[49m(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-5\u001b[39m, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m)\n\u001b[1;32m      3\u001b[0m total_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader) \u001b[38;5;241m*\u001b[39m epoches\n\u001b[1;32m      4\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m get_linear_schedule_with_warmup(optimizer, num_warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, num_training_steps\u001b[38;5;241m=\u001b[39mtotal_steps)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'AdamW' is not defined"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "Training",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "for _ in range(epoches):\n    model.train()\n    for batch in train_dataloader:\n        b_input_ids, b_labels, b_masks = batch\n        print(b_input_ids)\n        print(b_labels)\n        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_masks, labels=b_labels)\n        loss = outputs[0]\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        model.zero_grad()",
      "metadata": {
        "trusted": true
      },
      "execution_count": 27,
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoches):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m      4\u001b[0m         b_input_ids, b_labels, b_masks \u001b[38;5;241m=\u001b[39m batch\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "model.save_pretrained(\"./ner_model\")",
      "metadata": {
        "trusted": true
      },
      "execution_count": 28,
      "outputs": [
        {
          "ename": "<class 'NameError'>",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./ner_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ],
          "output_type": "error"
        }
      ]
    }
  ]
}